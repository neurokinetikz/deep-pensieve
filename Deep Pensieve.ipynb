{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Pensieve™\n",
    " \n",
    "#### An Expansive Deep Residual Super Resolution Information Maximizing Variational Auto-Encoder (EDSR+InfoVAE) \n",
    "\n",
    "with Dilated Convolutions, Group Normalization, Maximum Mean Discrepancy, Residual Bottleneck Attention, Efficient Sub-Pixel Convolution Super-Resolution, and Perceptual Similarity Loss\n",
    "\n",
    "<table><tr>\n",
    "<td><img src='https://s3.amazonaws.com/neurokinetikz/latent-animation-1540551741.4923084-final.gif'></td>\n",
    "<td><img src='https://s3.amazonaws.com/neurokinetikz/latent-animation-1540552110.470284-final.gif'></td>\n",
    "<td><img src=\"https://s3.amazonaws.com/neurokinetikz/1542113809.6163912-080.gif\"></td>\n",
    "<td><img src='https://s3.amazonaws.com/neurokinetikz/latent-animation-1540552122.395882-final.gif'></td>\n",
    "<td><img src='https://s3.amazonaws.com/neurokinetikz/latent-animation-1540551578.5925505-final.gif'></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Stage Variational Auto-Encoders for Coarse-to-Fine Image Generation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1705.07202\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/neurokinetikz/download-1.png\">\n",
    "\n",
    "Variational auto-encoder (VAE) is a powerful unsupervised learning framework for image generation. One drawback of VAE is that it generates blurry images due to its Gaussianity assumption and thus L2 loss. To allow the generation of high quality images by VAE, we increase the capacity of decoder network by employing residual blocks and skip connections, which also enable efficient optimization. To overcome the limitation of L2 loss, we propose to generate images in a multi-stage manner from coarse to fine. \n",
    "\n",
    "In the simplest case, the proposed multi-stage VAE divides the decoder into two components in which the second component generates refined images based on the course images generated by the first component. Since the second component is independent of the VAE model, it can employ other loss functions beyond the L2 loss and different model architectures. \n",
    "\n",
    "The proposed framework can be easily generalized to contain more than two components. Experiment results on the MNIST and CelebA datasets demonstrate that the proposed multi-stage VAE can generate sharper images as compared to those from the original VAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from libs import utils, gif\n",
    "from libs.group_norm import GroupNormalization\n",
    "from libs.variance_pooling import GlobalVariancePooling2D\n",
    "\n",
    "from keras.models import Model, load_model, model_from_json\n",
    "from keras.layers import Input, Flatten, Reshape, Add, Multiply, Activation, Lambda\n",
    "from keras.layers import Dense, Conv2D, DepthwiseConv2D, SeparableConv2D\n",
    "from keras.layers import MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D, UpSampling2D\n",
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "\n",
    "from keras_contrib.losses import DSSIMObjective\n",
    "from keras_contrib.layers.convolutional import SubPixelUpscaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = 'roadtrip'\n",
    "\n",
    "SIZE = 64\n",
    "SCALE_FACTOR = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNELS = 3\n",
    "\n",
    "FEATURES = SIZE*SIZE*CHANNELS\n",
    "FEATURES_2X = SCALE_FACTOR*SIZE*SCALE_FACTOR*SIZE*CHANNELS\n",
    "\n",
    "MODEL_NAME = DIRECTORY+'-'+str(SIZE)+'-'+str(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images\n",
    "imgs, xs, ys  = utils.load_images(directory=\"imgs/\"+DIRECTORY,rx=SIZE,ry=SIZE)\n",
    "imgs_2x, xs_2x, ys_2x = utils.load_images(directory=\"imgs/\"+DIRECTORY,rx=SCALE_FACTOR*SIZE,ry=SCALE_FACTOR*SIZE)\n",
    "\n",
    "# normalize pixels\n",
    "IMGS = imgs/127.5 - 1\n",
    "FLAT = np.reshape(IMGS,(-1,FEATURES))\n",
    "\n",
    "IMGS_2X = imgs_2x/127.5 - 1\n",
    "FLAT_2X = np.reshape(IMGS_2X,(-1,FEATURES_2X)) \n",
    "\n",
    "SAMPLES =  np.random.permutation(FLAT)[:9]\n",
    "SAMPLES_2X =  np.random.permutation(FLAT_2X)[:9]\n",
    "\n",
    "TOTAL_BATCH = IMGS.shape[0]\n",
    "\n",
    "# print shapes\n",
    "print(\"MODEL: \",MODEL_NAME)\n",
    "print(\"IMGS: \",IMGS.shape,IMGS_2X.shape)\n",
    "print(\"FLAT: \",FLAT.shape,FLAT_2X.shape)\n",
    "print(\"SAMPLES: \",SAMPLES.shape,SAMPLES_2X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3.amazonaws.com/neurokinetikz/Asset+4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Very Deep Convolutional Networks for Large-Scale Image Recognition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1409.1556\n",
    "\n",
    "<img src=\"https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_3/CascadingConvolutions.png\">\n",
    "\n",
    "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3×3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers.\n",
    "\n",
    "First, we incorporate three non-linear rectification layers instead of a single one, which makes the decision function more discriminative.\n",
    "\n",
    "Second, we decrease the number of parameters: assuming that both the input and the output of a\n",
    "three-layer 3 × 3 convolution stack has C channels, the stack is parametrised by (W) weights; at the same time, a single 7 × 7 conv. layer would require 81% more. This can be seen as imposing a regularisation on the 7 × 7 conv. filters, forcing them to have a decomposition through the 3 × 3 filters (with non-linearity injected in between)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/syncedreview/facebook-ai-proposes-group-normalization-alternative-to-batch-normalization-fb0699bffae7\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/neurokinetikz/groupnorm.png\">\n",
    "\n",
    "\n",
    "The mainstream normalization technique for almost all convolutional neural networks today is Batch Normalization (BN), which has been widely adopted in the development of deep learning. Proposed by Google in 2015, BN can not only accelerate a model’s converging speed, but also alleviate problems such as Gradient Dispersion in the deep neural network, making it easier to train models.\n",
    "\n",
    "Dr. Wu and Dr. He however argue in their paper Group Normalization that normalizing with batch size has limitations, as BN cannot ensure the model accuracy rate when the batch size becomes smaller. As a result, researchers today are normalizing with large batches, which is very memory intensive, and are avoiding using limited memory to explore higher-capacity models.\n",
    "\n",
    "Dr. Wu and Dr. He believe their new GN technique is a simple but effective alternative to BN. Specifically, GN divides channels — also referred to as feature maps that look like 3D chunks of data — into groups and normalizes the features within each group. GN only exploits the layer dimensions, and its computation is independent of batch sizes.\n",
    "\n",
    "The paper reports that GN had a 10.6% lower error rate than its BN counterpart for ResNet-50 in ImageNet with a batch size of 2 samples; and matched BN performance while outperforming other normalization techniques with a regular batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closed-form Expressions for Maximum Mean Discrepancy with Applications to Wasserstein Auto-Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1901.03227v1\n",
    "\n",
    "We propose using code normalization— a version of the batch normalization [16] applied at the code layer when training WAEs. This has the benefits of making the selection of width for the MMD kernel easier, reducing the training effort, and preventing outliers in the aggregate latent distribution.\n",
    "\n",
    "Code normalization shifts and scales codes to be in the “right” part of the space, namely where the target standard multivariate normal distribution lives, and we speculate that this reduces the training effort.\n",
    "\n",
    "... even if the neural network has converged to the target normal distribution, the gradient for a batch will not be zero but will have components in the direction of shifting and scaling the codes to reduce the MMD for a given batch. Code normalization directly takes care of this reduction, and allows the training process to spend its effort on improving the reconstruction error. Technically, this is achieved by projecting out the components of the gradient corresponding to shifting and scaling which is automatically achieved by normalization.\n",
    "\n",
    "Another benefit of code normalization is that it provides a solution to outlier insensitivity problem of the MMD penalty, described below. Indeed, scaling by the standard deviation (rather than by a robust surrogate) controls the tail behavior of the code distribution. Due to this control, the code distribution ends up having a light tail and no code falls too far away from the origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(x):\n",
    "    # set current layer\n",
    "    current_layer = Reshape((SIZE,SIZE,CHANNELS))(x)\n",
    "    \n",
    "    # convolution layers\n",
    "    for layer, n in enumerate(FILTERS):\n",
    "\n",
    "        # 3x3 convolution with group normalization + activation\n",
    "        current_layer = Conv2D(n,3,padding='SAME',kernel_initializer=INITIALIZER)(current_layer)\n",
    "        current_layer = GroupNormalization(groups=n,axis=-1)(current_layer)\n",
    "        current_layer = Activation(ACTIVATION)(current_layer)\n",
    "                \n",
    "        # dilation rate\n",
    "        dilation_rate = 1 if (layer+1 == len(FILTERS)) else 2     \n",
    "        \n",
    "        # dilated 3x3 convolution with group normalization + activation\n",
    "        current_layer = Conv2D(n,3,padding='SAME',dilation_rate=dilation_rate,kernel_initializer=INITIALIZER)(current_layer)\n",
    "        current_layer = GroupNormalization(groups=n,axis=-1)(current_layer)\n",
    "        current_layer = Activation(ACTIVATION)(current_layer)\n",
    "                \n",
    "        # max pooling\n",
    "        current_layer = MaxPooling2D()(current_layer)\n",
    "   \n",
    "    # 1x1 convolution on 4x4 feature maps\n",
    "    current_layer = Conv2D(int(LATENT_DIM/16),1,padding='SAME',kernel_initializer=INITIALIZER)(current_layer)\n",
    "    current_layer = GroupNormalization(groups=int(LATENT_DIM/16),axis=-1)(current_layer)\n",
    "    current_layer = Activation(ACTIVATION)(current_layer)\n",
    "        \n",
    "    # flatten\n",
    "    flat = Flatten()(current_layer)\n",
    "\n",
    "    # latent vector withe group normalization, no activation\n",
    "    z = Dense(LATENT_DIM,kernel_initializer=INITIALIZER)(flat)\n",
    "    z = GroupNormalization(groups=1,axis=-1,name='encoder')(z)\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dilated Residual Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1705.09914v1\n",
    "\n",
    "<img width=\"600\" src=\"https://s3.amazonaws.com/neurokinetikz/Screen+Shot+2019-01-22+at+10.06.28+AM.png\">\n",
    "\n",
    "Convolutional networks for image classification progressively reduce resolution until the image is represented by tiny feature maps in which the spatial structure of the scene is no longer discernible. Such loss of spatial acuity can limit image classification accuracy and complicate the transfer of the model to downstream applications that require detailed scene understanding. These problems can be alleviated by dilation, which increases the resolution of output feature maps without reducing the receptive field of individual neurons. We show that dilated residual networks (DRNs) outperform their non-dilated counterparts in image classification without increasing the model’s depth or complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receptive Field Calculator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://fomoro.com/projects/project/receptive-field-calculator\n",
    "\n",
    "<img width=\"600\" src=\"https://s3.amazonaws.com/neurokinetikz/download.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InfoVAE: Information Maximizing Variational Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1706.02262v3\n",
    "\n",
    "https://ermongroup.github.io/blog/a-tutorial-on-mmd-variational-autoencoders/\n",
    "\n",
    "\n",
    "Maximum mean discrepancy (MMD, (Gretton et al. 2007)) is based on the idea that two distributions are identical if and only if all their moments are the same. Therefore, we can define a divergence by measuring how “different” the moments of two distributions p(z) and q(z) are. MMD can accomplish this efficiently via the kernel embedding trick:\n",
    "\n",
    "A kernel can be intuitively interpreted as a function that measures the “similarity” of two samples. It has a large value when two samples are similar, and small when they are different. For example, the Gaussian kernel considers points that are close in Euclidean space to be “similar”. A rough intuition of MMD, then, is that if two distributions are identical, then the average “similarity” between samples from each distribution, should be identical to the average “similarity” between mixed samples from both distributions.\n",
    "\n",
    "\n",
    "<table><tr>\n",
    "<td><img src=\"https://s3.amazonaws.com/neurokinetikz/kl_latent.gif\" ></td>\n",
    "<td><img src=\"https://s3.amazonaws.com/neurokinetikz/mmd_latent.gif\" ></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kernel(x, y):\n",
    "    x_size = tf.shape(x)[0]\n",
    "    y_size = tf.shape(y)[0]\n",
    "    dim = tf.shape(x)[1]\n",
    "    tiled_x = tf.tile(tf.reshape(x, tf.stack([x_size, 1, dim])), tf.stack([1, y_size, 1]))\n",
    "    tiled_y = tf.tile(tf.reshape(y, tf.stack([1, y_size, dim])), tf.stack([x_size, 1, 1]))\n",
    "    return tf.exp(-tf.reduce_mean(tf.square(tiled_x - tiled_y), axis=2) / tf.cast(dim, tf.float32))\n",
    "\n",
    "def compute_mmd(x, y):\n",
    "    x_kernel = compute_kernel(x, x)\n",
    "    y_kernel = compute_kernel(y, y)\n",
    "    xy_kernel = compute_kernel(x, y)\n",
    "    return tf.reduce_mean(x_kernel) + tf.reduce_mean(y_kernel) - 2 * tf.reduce_mean(xy_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(y_true,y_pred):\n",
    "    epsilon = tf.random_normal(tf.stack([BATCH_SIZE, LATENT_DIM]))\n",
    "    latent_loss = compute_mmd(epsilon, y_pred)\n",
    "    return latent_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3.amazonaws.com/neurokinetikz/Asset+5.png\">\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deconvolution and Checkerboard Artifacts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://distill.pub/2016/deconv-checkerboard/\n",
    "\n",
    "When we have neural networks generate images, we often have them build them up from low resolution, high-level descriptions. This allows the network to describe the rough image and then fill in the details.\n",
    "\n",
    "In order to do this, we need some way to go from a lower resolution image to a higher one. We generally do this with the deconvolution operation. Roughly, deconvolution layers allow the model to use every point in the small image to “paint” a square in the larger one.\n",
    "\n",
    "Unfortunately, deconvolution can easily have “uneven overlap,” putting more of the metaphorical paint in some places than others. In particular, deconvolution has uneven overlap when the kernel size (the output window size) is not divisible by the stride (the spacing between points on the top). While the network could, in principle, carefully learn weights to avoid this  — as we’ll discuss in more detail later — in practice neural networks struggle to avoid it completely.\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/neurokinetikz/download-2.png\">\n",
    "\n",
    "To avoid these artifacts, we’d like an alternative to regular deconvolution (“transposed convolution”). Unlike deconvolution, this approach to upsampling shouldn’t have artifacts as its default behavior. Ideally, it would go further, and be biased against such artifacts.\n",
    "\n",
    "One approach is to separate out upsampling to a higher resolution from convolution to compute features. For example, you might resize the image (using nearest-neighbor interpolation or bilinear interpolation) and then do a convolutional layer. This seems like a natural approach, and roughly similar methods have worked well in image super-resolution.\n",
    "\n",
    "Our experience has been that nearest-neighbor resize followed by a convolution works very well, in a wide variety of contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization in the final layer of generative networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1805.07389v1\n",
    "\n",
    "<img width=300 src=\"https://s3.amazonaws.com/neurokinetikz/Screen+Shot+2019-01-21+at+12.38.07+PM.png\">\n",
    "\n",
    "Variational Autoencoders (VAE) use a latent vector Z as input with the output being an image. All of these forms of generative networks must at some point constrain the activations to pixel values. For the case of colour images, the network must reduce down to three channels and the values would normally be constrained to be integers in the range [0, 255]. To constrain the pixel values to be in an appropriate range the activation of choice is the tanh although a sigmoid could also be used. The tanh function takes an unbounded real number and constrains it to the real number range [−1, 1]. However as tanh is a non-linear function and we see from Figure 1 that inputs outside the range [−2, 2] will be saturated. When converted to an 8-bit image these saturated values will convert to colour values of 0 and 255. For most real images the pixel values will be well spread between [0, 255]. If the generator network is to produce realistic looking images, it should naturally produce images that have pixel values well spread between [−1, 1] on the output of the tanh. The network previous to the tanh should be aiming to produce activations with a mean close to zero and standard deviation close to one, to ensure a good spread of values entering the tanh. It is still reasonable for some activations of the tanh to saturate. Many real-world colour values will be 0 or 255. Placing a BN layer between the final activations and the tanh allows the activations earlier in the network to be less constrained. The BN will shift and spread/condense the values to a range that suits the tanh function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(z,z_g):\n",
    "    # inflate\n",
    "    inflate = Dense(LATENT_DIM,name='generator')\n",
    "    current_layer = inflate(z) ; generator = inflate(z_g)\n",
    "    \n",
    "    # reshape\n",
    "    reshape = Reshape((4,4,int(LATENT_DIM/16)))\n",
    "    current_layer = reshape(current_layer) ; generator = reshape(generator)\n",
    "    \n",
    "    # 1x1 convolution \n",
    "    c0 = Conv2D(int(LATENT_DIM/16),1,padding=\"SAME\",kernel_initializer=INITIALIZER)\n",
    "    g0 = GroupNormalization(groups=int(LATENT_DIM/16),axis=-1)\n",
    "    a0 = Activation(ACTIVATION)\n",
    "        \n",
    "    current_layer = c0(current_layer) ; generator = c0(generator)\n",
    "    current_layer = g0(current_layer) ; generator = g0(generator)\n",
    "    current_layer = a0(current_layer) ; generator = a0(generator)\n",
    "        \n",
    "    # reverse the filters\n",
    "    filters = FILTERS[::-1]\n",
    "    \n",
    "    # build layers\n",
    "    for layer, n in enumerate(filters):\n",
    "\n",
    "        # upsample\n",
    "        u = SubPixelUpscaling(scale_factor=2)\n",
    "        current_layer = u(current_layer) ; generator = u(generator)\n",
    "\n",
    "        # dilation rate\n",
    "        dilation_rate = 1 if (layer == 0) else 2\n",
    "            \n",
    "        # dilated 3x3 convolution with group normalization + activation\n",
    "        c1 = Conv2D(n,3,padding='SAME',dilation_rate=dilation_rate,kernel_initializer=INITIALIZER)\n",
    "        g1 = GroupNormalization(groups=n,axis=-1)\n",
    "        a1 = Activation(ACTIVATION)\n",
    "        \n",
    "        current_layer = c1(current_layer) ; generator = c1(generator)\n",
    "        current_layer = g1(current_layer) ; generator = g1(generator)\n",
    "        current_layer = a1(current_layer) ; generator = a1(generator)\n",
    "        \n",
    "        # 3x3 convolution with group normalization + activation\n",
    "        c2 = Conv2D(n,3,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "        g2 = GroupNormalization(groups=n,axis=-1)\n",
    "        a2 = Activation(ACTIVATION)\n",
    "        \n",
    "        \n",
    "        current_layer = c2(current_layer) ; generator = c2(generator)\n",
    "        current_layer = g2(current_layer) ; generator = g2(generator)\n",
    "        current_layer = a2(current_layer) ; generator = a2(generator)\n",
    "            \n",
    "    # output convolution\n",
    "    c = Conv2D(CHANNELS,1,padding='SAME', activation='tanh',name='decoder_dssim')\n",
    "    current_layer = c(current_layer) ; generator = c(generator)\n",
    "    \n",
    "    # flatten\n",
    "    flatten = Flatten(name='decoder')\n",
    "    decoder_loss = flatten(current_layer)\n",
    "    \n",
    "    return current_layer, generator, decoder_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refiner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"https://s3.amazonaws.com/neurokinetikz/Asset%2B10-100.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Deep Residual Networks for Single Image Super-Resolution (EDSR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1707.02921\n",
    "\n",
    "<img width=500 src=\"https://s3.amazonaws.com/neurokinetikz/Screen+Shot+2018-11-18+at+8.54.43+AM.png\">\n",
    "\n",
    "Recently, the powerful capability of deep neural networks has led to dramatic improvements in SR. Since Dong et al. [4, 5] first proposed a deep learning-based SR method, various CNN architectures have been studied for SR. Kim et al. [11, 12] first introduced the residual network for training much deeper network architectures and achieved superior performance. In particular, they showed that skip connection and recursive convolution alleviate the burden of carrying identity information in the super-resolution network. Similarly to [20], Mao et al. [16] tackled the general image restoration problem with encoder-decoder networks and symmetric skip connections. In [16], they argue that those nested skip connections provide fast and improved convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine(x,x_g,name='refiner'):\n",
    "    # 1x1 channel convolution\n",
    "    c1 = Conv2D(CHANNELS,1,padding='SAME')\n",
    "    current_layer = c1(x) ; generator = c1(x_g)\n",
    "    \n",
    "    # shortcut\n",
    "    shortcut = current_layer; shortcut_g = generator\n",
    "    \n",
    "    # reshape convolution\n",
    "    c2 = Conv2D(R_FILTERS,3,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "    current_layer = c2(x) ; generator = c2(x_g)\n",
    "\n",
    "    # residual layers\n",
    "    for i in range(R_LAYERS):\n",
    "        current_layer, generator = residual(current_layer, generator, R_ATTENTION)\n",
    "    \n",
    "    # output convolution\n",
    "    c3 = Conv2D(CHANNELS,1,padding='SAME')\n",
    "    current_layer = c3(current_layer); generator = c3(generator)\n",
    "    \n",
    "    # merge shortcut\n",
    "    merge = Add()\n",
    "    current_layer = merge([current_layer, shortcut]) ; generator = merge([generator, shortcut_g])\n",
    "    \n",
    "    # output activation\n",
    "    activation = Activation('tanh',name=name+'_dssim')\n",
    "    current_layer = activation(current_layer) ; generator = activation(generator)\n",
    "    \n",
    "    # flatten\n",
    "    flatten = Flatten(name=name)\n",
    "    refiner_loss = flatten(current_layer)\n",
    "    \n",
    "    return current_layer, generator, refiner_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3.amazonaws.com/neurokinetikz/download-3.png\">\n",
    "\n",
    "\n",
    "We remove the batch normalization layers from our network as Nah et al.[19] presented in their image deblurring work. Since batch normalization layers normalize the features, they get rid of range flexibility from networks by normalizing the features, it is better to remove them. We experimentally show that this simple modification increases the performance substantially as detailed in\n",
    "\n",
    "Furthermore, GPU memory usage is also sufficiently reduced since the batch normalization layers consume the same amount of memory as the preceding convolutional layers. Our baseline model without batch normalization layer saves approximately 40% of memory usage during training, compared to SRResNet. Consequently, we can build up a larger model that has better performance than conventional ResNet structure under limited computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual(x,x_g,attention=False):\n",
    "    # current layer\n",
    "    current_layer = x ; generator = x_g\n",
    "\n",
    "    # shortcuts\n",
    "    shortcut = current_layer ; shortcut_g = generator\n",
    "\n",
    "    # 3x3 convolution\n",
    "    c1 = Conv2D(R_FILTERS,3,padding='SAME',activation=ACTIVATION,kernel_initializer=INITIALIZER)\n",
    "    current_layer = c1(current_layer) ; generator = c1(generator)\n",
    "\n",
    "    # dilated 3x3 convolution\n",
    "    dilation_rate = 1 #int(SIZE/32)\n",
    "    c2 = Conv2D(R_FILTERS,3,padding='SAME',dilation_rate=dilation_rate,kernel_initializer=INITIALIZER)\n",
    "    current_layer = c2(current_layer) ; generator = c2(generator)\n",
    "    \n",
    "    # residual scaling\n",
    "    scale = Lambda(lambda x: x * R_SCALING)\n",
    "    current_layer = scale(current_layer) ; generator = scale(generator)\n",
    "    \n",
    "    # residual attention\n",
    "    if(attention):\n",
    "        current_layer, generator = residual_attention(current_layer,generator)\n",
    "    \n",
    "    # merge shortcut\n",
    "    merge = Add()\n",
    "    current_layer = merge([current_layer, shortcut]) ; generator = merge([generator, shortcut_g])\n",
    "\n",
    "    return current_layer, generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Bottleneck Attention Module (RBAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3.amazonaws.com/neurokinetikz/Asset+8.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_attention(x,x_g):\n",
    "    # current layer\n",
    "    current_layer = x ; generator = x_g\n",
    "    \n",
    "    # shortcuts\n",
    "    shortcut = current_layer ; shortcut_g = generator\n",
    "    \n",
    "    # channel attention\n",
    "    ca, ca_g = channel_attention(current_layer, generator)\n",
    "    \n",
    "    # spatial attention\n",
    "    sa, sa_g = spatial_attention(current_layer, generator)\n",
    "    \n",
    "    # fuse channel and spatial attention\n",
    "    fuse = Add()\n",
    "    current_layer = fuse([ca,sa]); generator = fuse([ca_g,sa_g])\n",
    "    \n",
    "    # sigmoid activation\n",
    "    s = Activation(\"sigmoid\")\n",
    "    current_layer = s(current_layer) ; generator = s(generator)\n",
    "    \n",
    "    # merge fused attention with shortcut\n",
    "    m = Multiply()\n",
    "    current_layer = m([current_layer,shortcut]) ; generator = m([generator, shortcut_g])\n",
    "    \n",
    "    return current_layer, generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Super-Resolution Using Very Deep Residual Channel Attention Networks (RCAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1807.02758v2\n",
    "\n",
    "<img width=\"600\" src=\"https://s3.amazonaws.com/neurokinetikz/Screen+Shot+2018-12-16+at+11.12.08+AM.png\">\n",
    "\n",
    "To make a further step, we propose channel attention (CA) mechanism to adaptively rescale each channel-wise feature by modeling the interdependencies across feature channels. Such CA mechanism allows our proposed network to concentrate on more useful channels and enhance discriminative learning ability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Attention Module for Single Image Super-Resolution (RAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1811.12043\n",
    "\n",
    "<img width=\"500\" src=\"https://s3.amazonaws.com/neurokinetikz/Screen+Shot+2018-12-07+at+8.51.39+AM.png\">\n",
    "\n",
    "In this paper, we propose a new attention method, which is composed of new channel-wise and spatial attention mechanisms optimized for SR and a new fused attention to combine them. Based on this, we propose a new residual attention module (RAM) and a SR network using RAM (SRRAM). We provide in-depth experimental analysis of different attention mechanisms in SR. It is shown that the proposed method can construct both deep and lightweight SR networks showing improved performance in comparison to existing state-of-the-art methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_attention(x,x_g):\n",
    "    # current layer\n",
    "    current_layer = x ; generator = x_g\n",
    "    \n",
    "    # global variance pooling\n",
    "    gvp = GlobalVariancePooling2D(); reshape = Reshape((1,1,R_FILTERS))\n",
    "    current_layer = reshape(gvp(current_layer)); generator = reshape(gvp(generator))\n",
    "    \n",
    "    # squeeze & excitation\n",
    "    squeeze = Conv2D(int(R_FILTERS/R_REDUCTION),1,padding='SAME',activation=ACTIVATION,kernel_initializer=INITIALIZER)\n",
    "    current_layer = squeeze(current_layer); generator = squeeze(generator);\n",
    "    \n",
    "    # scaling\n",
    "    c2 = Conv2D(R_FILTERS,1,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "    current_layer = c2(current_layer) ; generator = c2(generator)\n",
    "    \n",
    "    # group normalization\n",
    "    gn = GroupNormalization(groups=1,axis=-1)\n",
    "    current_layer = gn(current_layer); generator = gn(generator)\n",
    "        \n",
    "    return current_layer, generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BAM: Bottleneck Attention Module "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1807.06514v2\n",
    "\n",
    "<img width=\"600\" src=\"https://s3.amazonaws.com/neurokinetikz/Screen+Shot+2018-12-16+at+10.15.09+AM.png\">\n",
    "\n",
    "In this work, we focus on the effect of attention in general deep neural networks. We propose a simple and effective attention module, named Bottleneck Attention Module (BAM), that can be integrated with any feed-forward convolutional neural networks. Our module infers an attention map along two separate pathways, channel and spatial. We place our module at each bottleneck of models where the downsampling of feature maps occurs. Our module constructs a hierarchical attention at bottlenecks with a number of parameters and it is trainable in an end-to-end manner jointly with any feed-forward models.\n",
    "\n",
    "As the channels of feature maps can be regarded as feature detectors, the two branches (spatial and channel) explicitly learn ‘what’ and ‘where’ to focus on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_attention(x,x_g):\n",
    "    # current layer\n",
    "    current_layer = x ; generator = x_g\n",
    "    \n",
    "    # 1x1 convolution\n",
    "    c1 = Conv2D(int(R_FILTERS/R_REDUCTION),1,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "    current_layer = c1(current_layer); generator = c1(generator)\n",
    "\n",
    "    # dilated convolution\n",
    "    c2 = Conv2D(int(R_FILTERS/R_REDUCTION),3,dilation_rate=R_DILATION,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "    current_layer = c2(current_layer) ; generator = c2(generator)\n",
    "\n",
    "    # dilated convolution\n",
    "    c3 = Conv2D(int(R_FILTERS/R_REDUCTION),3,dilation_rate=R_DILATION,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "    current_layer = c3(current_layer) ; generator = c3(generator)\n",
    "\n",
    "    # 1x1 convolution\n",
    "    c4 = Conv2D(1,1,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "    current_layer = c4(current_layer); generator = c4(generator)\n",
    "    \n",
    "    # group normalization\n",
    "    gn = GroupNormalization(groups=1,axis=-1)\n",
    "    current_layer = gn(current_layer); generator = gn(generator)\n",
    "    \n",
    "    return current_layer, generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1609.05158\n",
    "\n",
    "<img width=\"75%\" src=\"https://s3.amazonaws.com/neurokinetikz/Screen+Shot+2018-11-23+at+10.26.45+AM.png\">\n",
    "\n",
    "Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. \n",
    "\n",
    "In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. \n",
    "\n",
    "We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(x,x_g,img_g,name='super'):\n",
    "    \n",
    "    # current layer\n",
    "    current_layer = x ; generator = x_g; img_generator = img_g\n",
    "    \n",
    "    # convolution\n",
    "    c1 = Conv2D(R_FILTERS*SCALE_FACTOR*SCALE_FACTOR,3,padding='SAME',activation=ACTIVATION,kernel_initializer=INITIALIZER)\n",
    "    current_layer = c1(current_layer) ; generator = c1(generator) ; img_generator = c1(img_generator)\n",
    "    \n",
    "    # sub-pixel upscaling\n",
    "    upscale = SubPixelUpscaling(scale_factor=SCALE_FACTOR)\n",
    "    current_layer = upscale(current_layer); generator = upscale(generator); img_generator = upscale(img_generator)\n",
    "    \n",
    "    # In practice, it is useful to have a second convolution layer after the \n",
    "    # SubPixelUpscaling layer to speed up the learning process.\n",
    "    c2 = Conv2D(R_FILTERS*SCALE_FACTOR*SCALE_FACTOR,3,padding='SAME',activation=ACTIVATION,kernel_initializer=INITIALIZER)\n",
    "    current_layer = c2(current_layer) ; generator = c2(generator); img_generator = c2(img_generator)\n",
    "    \n",
    "    # output convolution\n",
    "    c3 = Conv2D(CHANNELS,1,padding='SAME',activation='tanh',name=name+\"_dssim\")\n",
    "    current_layer = c3(current_layer); generator = c3(generator); img_generator = c3(img_generator)\n",
    "    \n",
    "    # flatten\n",
    "    flatten = Flatten(name=name)\n",
    "    upscale_loss = flatten(current_layer)\n",
    "    \n",
    "    return current_layer, generator, img_generator, upscale_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1511.07289\n",
    "\n",
    "<img width=\"300\" style=\"float:left;\" src=\"https://blogs.mathworks.com/deep-learning/files/2017/12/defining_elu_layer_01.png\">\n",
    "\n",
    "We introduce the \"exponential linear unit\" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. \n",
    "\n",
    "In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. \n",
    "\n",
    "Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default activation\n",
    "ACTIVATION  = 'elu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters in Action! Part II — Weight Initializers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/hyper-parameters-in-action-part-ii-weight-initializers-35aee1a28404\n",
    "\n",
    "<img width=\"300\" style=\"float:left;\" src=\"https://cdn-images-1.medium.com/max/1600/1*WLUL_bcjsNK9sXNw6nC-cg.png\">\n",
    "\n",
    "If you dug a little bit deeper, you’ve likely also found out that one should use Xavier / Glorot initialization if the activation function is a Tanh, and that He initialization is the recommended one if the activation function is a ReLU.\n",
    "\n",
    "In summary, for a ReLU activated network, the He initialization scheme using an Uniform distribution is a pretty good choice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializers\n",
    "INITIALIZER = 'he_uniform'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to Generate Images with Perceptual Similarity Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1511.06409\n",
    "\n",
    "<img width=\"200\" style=\"float:left;\" src=\"https://s3.amazonaws.com/neurokinetikz/download-4.png\">\n",
    "\n",
    "In this paper, we explore loss functions that, unlike MSE, MAE, and likelihoods, are grounded in human perceptual judgments. We show that these perceptual losses lead to representations are superior to other methods, both with respect to reconstructing given images, and generating novel ones. This superiority is demonstrated both in quantitative studies and human judgements ... We (also) demonstrate that perceptual losses yield a convincing win when applied to a state-of-the-art architecture for single image super-resolution.\n",
    "\n",
    "As observed in the deterministic case, MS-SSIM is better at capturing fine details than either MSE or MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gifit(epoch=None):\n",
    "    if (epoch % GIF_STEPS == 0):\n",
    "        print('saving gif ...')\n",
    "        z,y,yf,img,imgf,imgs,imgsf = AUTOENCODER.predict_on_batch(SAMPLES)\n",
    "        img = np.clip(127.5*(imgs+1).reshape((-1, SCALE_FACTOR*SIZE, SCALE_FACTOR*SIZE, CHANNELS)), 0, 255)\n",
    "        RECONS.append(utils.montage(img).astype(np.uint8))\n",
    "        \n",
    "def saveit(epoch=None):\n",
    "    if ((epoch > 0) and (epoch % MODEL_STEPS == 0)):\n",
    "        print('saving model ...')\n",
    "        AUTOENCODER.save(MODEL_NAME+'-autoencoder-model.h5')\n",
    "        ENCODER.save(MODEL_NAME+'-encoder-model.h5')\n",
    "        DECODER.save(MODEL_NAME+'-generator-model.h5')\n",
    "        SUPER.save(MODEL_NAME+'-super-model.h5')\n",
    "        SUPERSIZER.save(MODEL_NAME+'-supersizer-model.h5')\n",
    "        print('done')\n",
    "       \n",
    "        \n",
    "# callbacks\n",
    "giffer = LambdaCallback(on_epoch_end=lambda epoch, logs: gifit(epoch))\n",
    "saver = LambdaCallback(on_epoch_end=lambda epoch, logs: saveit(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE  = 4\n",
    "SAMPLES =  np.random.permutation(FLAT)[:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder/Decoder\n",
    "if (SIZE == 256):\n",
    "    FILTERS = [64,96,128,160,192,224]\n",
    "    \n",
    "elif (SIZE == 128):\n",
    "    FILTERS = [64,128,192,256,320]\n",
    "    \n",
    "elif (SIZE == 64):\n",
    "    FILTERS = [64,128,192,256]\n",
    "    \n",
    "elif (SIZE == 32):\n",
    "    FILTERS = [64,96,128]\n",
    "    \n",
    "# Residuals\n",
    "R_LAYERS  = 16\n",
    "R_FILTERS = 64\n",
    "R_SCALING = 0.01\n",
    "\n",
    "# Attention Modules\n",
    "R_ATTENTION = True\n",
    "R_REDUCTION = 4\n",
    "R_DILATION = 4\n",
    "\n",
    "# Latent dimension size\n",
    "LATENT_DIM = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input\n",
    "X = Input(shape=(FEATURES,))\n",
    "\n",
    "# encode\n",
    "Z = encode(X)\n",
    "\n",
    "# decoder input\n",
    "Z_G = Input(shape=(LATENT_DIM,))\n",
    "\n",
    "# decode\n",
    "Y, Y_G, Y_F = decode(Z,Z_G)\n",
    "\n",
    "# refine\n",
    "IMG, IMG_G, IMG_F = refine(Y,Y_G)\n",
    "\n",
    "# supersizer input\n",
    "X_G = Input(shape=(SIZE,SIZE,CHANNELS))\n",
    "\n",
    "# supersize\n",
    "IMG_S, IMG_G_S, X_G_S, IMG_S_F = upsample(IMG,IMG_G,X_G)\n",
    "\n",
    "\n",
    "# model definitions\n",
    "ENCODER = Model(inputs=[X], outputs=[Z])\n",
    "DECODER = Model(inputs=[Z_G], outputs=[IMG_G])\n",
    "SUPER = Model(inputs=[Z_G], outputs=[IMG_G_S])\n",
    "SUPERSIZER = Model(inputs=[X_G], outputs=[X_G_S])\n",
    "\n",
    "# define optimizer\n",
    "ADAM = optimizers.Adam(amsgrad=True)\n",
    "\n",
    "# compile models\n",
    "ENCODER.compile(optimizer=ADAM,loss='mse')\n",
    "DECODER.compile(optimizer=ADAM,loss='mae')\n",
    "SUPER.compile(optimizer=ADAM,loss='mae')\n",
    "SUPERSIZER.compile(optimizer=ADAM,loss='mae')\n",
    "\n",
    "\n",
    "# define autoencoder\n",
    "AUTOENCODER = Model(inputs=[X], outputs=[Z,Y,Y_F,IMG,IMG_F,IMG_S,IMG_S_F])\n",
    "\n",
    "# define losses\n",
    "losses = {'encoder':vae_loss,\n",
    "          'decoder':'mse',\n",
    "          'decoder_dssim':DSSIMObjective(),\n",
    "          'refiner':'mae',\n",
    "          'refiner_dssim':DSSIMObjective(),\n",
    "          'super':'mse',\n",
    "          'super_dssim':DSSIMObjective()}\n",
    "\n",
    "# compile model\n",
    "AUTOENCODER.compile(optimizer=ADAM,loss=losses)\n",
    "\n",
    "# print summary\n",
    "AUTOENCODER.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "RECONS = []\n",
    "\n",
    "START_EPOCH = 0\n",
    "EPOCHS      = 101\n",
    "\n",
    "MODEL_STEPS = 1000\n",
    "GIF_STEPS   = 1\n",
    "\n",
    "\n",
    "# fit model\n",
    "AUTOENCODER.fit(x=FLAT,\n",
    "                y=[FLAT,IMGS,FLAT,IMGS,FLAT,IMGS_2X,FLAT_2X],\n",
    "                batch_size=BATCH_SIZE,\n",
    "                epochs=EPOCHS,\n",
    "                callbacks=[giffer,saver],\n",
    "                initial_epoch=START_EPOCH)\n",
    "\n",
    "# save training gif\n",
    "gif.build_gif(RECONS, saveto=MODEL_NAME+'-final'+ \"-\"+str(time.time())+'.gif')\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reanimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.youtube.com/watch?v=grEi3uRlSb4\n",
    "<table><tr>\n",
    "<td><img src=\"https://s3.amazonaws.com/neurokinetikz/1542113809.6163912-109.gif\"></td>\n",
    "<td><img src=\"https://s3.amazonaws.com/neurokinetikz/1542113809.6163912-100.gif\"></td>\n",
    "<td><img src=\"https://s3.amazonaws.com/neurokinetikz/1542113809.6163912-080.gif\"></td>\n",
    "</tr></table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_latents(n_imgs=3,steps=30):\n",
    "    rimgs = np.random.permutation(FLAT)[:n_imgs]\n",
    "    rimgs = np.append(rimgs, [rimgs[0]],axis=0)\n",
    "    latent_animation(rimgs,steps,filename=str(time.time()))\n",
    "\n",
    "def latent_animation(imgs=None,steps=None,filename=\"latent-animation\"):\n",
    "    animate(generate(get_latents(imgs,steps),filename),filename)\n",
    "    \n",
    "def get_latents(imgs,steps):\n",
    "    # get latent encodings for images\n",
    "    print('encoding latent vectors ...')\n",
    "    latents = []\n",
    "    for index,img in enumerate(imgs):\n",
    "        img = np.reshape(img,(-1,FEATURES))\n",
    "        latent = ENCODER.predict_on_batch(img)\n",
    "        latents.append(latent)\n",
    "\n",
    "    # calculate latent path\n",
    "    print('calculating latent path ...')\n",
    "    latent_path = []\n",
    "    for i in range(len(latents)-1):\n",
    "        # get latent vectors\n",
    "        l1 = latents[i] ; l2 = latents[i+1]\n",
    "\n",
    "        # calculate latent distance\n",
    "        image_distance = l2 - l1\n",
    "\n",
    "        # create the latent path\n",
    "        for j in range(steps):\n",
    "            latent_path.append(l1 + j*image_distance/steps)\n",
    "        latent_path.append(l2)\n",
    "    \n",
    "    return latent_path\n",
    "       \n",
    "    \n",
    "def generate(latent_path,filename=None):\n",
    "     # reconstruct images along the path\n",
    "    latent_path = np.reshape(latent_path,(-1,LATENT_DIM))\n",
    "    \n",
    "    print('decoding ...')\n",
    "    recons = DECODER.predict_on_batch(latent_path)\n",
    "    \n",
    "    if(filename != None):\n",
    "        print('saving decoder gif')\n",
    "        build_gif(np.asarray(recons),SIZE,filename)\n",
    "    \n",
    "    return recons\n",
    "    \n",
    "def animate(recons,filename=None):\n",
    "    print('supersizing ...')\n",
    "    chunks = SUPERSIZER.predict_on_batch(recons[:10])\n",
    "    for i in range(10,len(recons)-10,10):\n",
    "        s2 = SUPERSIZER.predict_on_batch(recons[i:i+10])\n",
    "        chunks = np.concatenate([chunks,s2])\n",
    "    \n",
    "#     print('saving supersizer gif')\n",
    "#     build_gif(chunks,SIZE*SCALE_FACTOR,filename+\"-\"+str(SCALE_FACTOR)+\"x\")\n",
    "   \n",
    "    # done\n",
    "#     print(filename)\n",
    "    \n",
    "    return chunks\n",
    "    \n",
    "def build_gif(recons,size,filename='latent-animation'):\n",
    "    final = np.clip((127.5*(recons+1)).reshape((-1,size,size,CHANNELS)),0,255)\n",
    "    gif.build_gif([utils.montage([r]).astype(np.uint8) for r in final], saveto=filename+\".gif\",dpi=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SCALE_FACTOR=2\n",
    "GRID = 16\n",
    "N_IMGS = 3\n",
    "STEPS = 50\n",
    "\n",
    "for j in range(1):\n",
    "    grid=[]\n",
    "    for i in range(GRID):\n",
    "        imgs = np.random.permutation(FLAT)[:N_IMGS]\n",
    "        imgs = np.append(imgs, [imgs[0]],axis=0)\n",
    "\n",
    "        latent_path = get_latents(imgs,STEPS)\n",
    "        recons = animate(generate(latent_path),'')\n",
    "        final = np.clip((127.5*(recons+1)).reshape((-1,SIZE*SCALE_FACTOR,SIZE*SCALE_FACTOR,CHANNELS)),0,255)\n",
    "        grid.append(final)\n",
    "\n",
    "    rs = []\n",
    "    for img in grid:\n",
    "        rs.append([frame for frame in img])\n",
    "    rs = np.moveaxis(grid,1,0)\n",
    "    gif.build_gif([utils.montage(r).astype(np.uint8) for r in rs], saveto=str(time.time())+\".gif\",dpi=72)\n",
    "\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LATENT_DIM=512\n",
    "for i in range(100):\n",
    "    random_latents(3,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imgs =  np.random.permutation(FLAT)\n",
    "t = str(time.time())\n",
    "for i in range(TOTAL_BATCH):\n",
    "    print(i)\n",
    "    latent_animation([imgs[i],imgs[i+1]],100,filename=t+'-'+ ('%03d' % i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'roadtrip-128-1548020779.1727364'\n",
    "\n",
    "print('loading encoder ...', MODEL_NAME)\n",
    "ENCODER = load_model(MODEL_NAME+'-encoder-model.h5')\n",
    "\n",
    "print('loading decoder ...')\n",
    "DECODER = load_model(MODEL_NAME+'-generator-model.h5', custom_objects={'R_SCALING':R_SCALING,'GlobalVariancePooling2D':GlobalVariancePooling2D})\n",
    "\n",
    "print('loading supersizer ...')\n",
    "SUPERSIZER = load_model(MODEL_NAME+'-supersizer-model.h5')\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img width=600 src=\"https://s3.amazonaws.com/neurokinetikz/Screen+Shot+2018-11-13+at+12.17.10+PM.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(index=0):\n",
    "    \n",
    "    # input\n",
    "    x = np.reshape(FLAT[index],(-1,FEATURES))\n",
    "    z = ENCODER.predict_on_batch(x)\n",
    "    \n",
    "    # output\n",
    "    img = np.reshape(DECODER.predict_on_batch(z),(-1,FEATURES))\n",
    "    img_s = np.reshape(SUPER.predict_on_batch(z),(-1,FEATURES*SCALE_FACTOR*SCALE_FACTOR))\n",
    "    \n",
    "    # reference\n",
    "    ref = IMGS[index]/2 + .5\n",
    "    ref_s = IMGS_2X[index]/2 + .5\n",
    "    \n",
    "    # denormalize\n",
    "    img = np.reshape(img/2 + .5,(SIZE,SIZE,CHANNELS))\n",
    "    img_s= np.reshape(img_s/2 + .5,(SCALE_FACTOR*SIZE,SCALE_FACTOR*SIZE,CHANNELS))\n",
    "    \n",
    "    # print scores\n",
    "    print(\"PSNR: %.3f %.3f <> MS-SSIM: %.3f %.3f\" % ((utils.psnr(ref,img)),\n",
    "                                                     (utils.psnr(ref_s,img_s)),\n",
    "                                           (utils.MultiScaleSSIM(np.reshape(ref,(1,SIZE,SIZE,CHANNELS)),\n",
    "                                                                 np.reshape(img,(1,SIZE,SIZE,CHANNELS)),\n",
    "                                                                 max_val=1.)),\n",
    "                                            (utils.MultiScaleSSIM(np.reshape(ref_s,(1,SCALE_FACTOR*SIZE,SCALE_FACTOR*SIZE,CHANNELS)),\n",
    "                                                                 np.reshape(img_s,(1,SCALE_FACTOR*SIZE,SCALE_FACTOR*SIZE,CHANNELS)),\n",
    "                                                                 max_val=1.))\n",
    "                                               ))\n",
    "    \n",
    "    # show images\n",
    "    utils.showImagesHorizontally(images=[ref,img])\n",
    "    utils.showImagesHorizontally(images=[ref_s,img_s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruct(random.randint(0,TOTAL_BATCH-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model & Continue Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "RECONS = []\n",
    "MODEL_NAME = 'roadtrip-128-1548020779.1727364'\n",
    "MODEL_STEPS = 1\n",
    "GIF_STEPS = 1\n",
    "\n",
    "print('loading model ...')\n",
    "AUTOENCODER = load_model(MODEL_NAME+'-autoencoder-model.h5', \n",
    "                         custom_objects={'vae_loss': vae_loss,\n",
    "                                         'R_SCALING':R_SCALING,\n",
    "                                         'DSSIMObjective':DSSIMObjective(),\n",
    "                                         'GlobalVariancePooling2D':GlobalVariancePooling2D})\n",
    "\n",
    "# optimizer\n",
    "ADAM = optimizers.Adam(amsgrad=True)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "print('loading encoder ...', MODEL_NAME)\n",
    "ENCODER = load_model(MODEL_NAME+'-encoder-model.h5')\n",
    "ENCODER.summary()\n",
    "\n",
    "print('loading decoder ...')\n",
    "DECODER = load_model(MODEL_NAME+'-generator-model.h5', custom_objects={'R_SCALING':R_SCALING,'GlobalVariancePooling2D':GlobalVariancePooling2D})\n",
    "DECODER.summary()\n",
    "\n",
    "print('loading supersizer ...')\n",
    "SUPERSIZER = load_model(MODEL_NAME+'-supersizer-model.h5')\n",
    "SUPERSIZER.summary()\n",
    "\n",
    "# ENCODER = Model(inputs=[AUTOENCODER.input],outputs=[AUTOENCODER.get_layer(\"encoder\").output])\n",
    "# ENCODER.compile(optimizer=ADAM,loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Z = ENCODER.get_layer('encoder').output\n",
    "# Y = DECODER.get_layer('generator').output\n",
    "# Y_F = DECODER.get_layer('decoder_dssim').output\n",
    "# IMG = DECODER.get_layer('refiner').output\n",
    "# IMG_F = DECODER.get_layer('refiner_dssim').output\n",
    "# IMG_S = SUPERSIZER.get_layer('super').output\n",
    "# IMG_S_F = SUPERSIZER.get_layer('super_dssim').output\n",
    "\n",
    "\n",
    "\n",
    "VAE = Model(inputs=[ENCODER.input],outputs=[SUPERSIZER(DECODER(ENCODER.output))])\n",
    "\n",
    "X = VAE.input\n",
    "\n",
    "Z = VAE.get_layer('encoder').output\n",
    "\n",
    "IMG = VAE.get_layer('model_2').get_layer('refiner_dssim').output\n",
    "\n",
    "IMG_S = VAE.output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "AUTOENCODER = Model(inputs=[X,VAE.get_layer('model_2').get_layer('generator').input],outputs=[Z, IMG_S, IMG])\n",
    "\n",
    "# define optimizer\n",
    "ADAM = optimizers.Adam(amsgrad=True)\n",
    "\n",
    "\n",
    "# # define autoencoder\n",
    "# AUTOENCODER = Model(inputs=[X], outputs=[Z,Y,Y_F,IMG,IMG_F,IMG_S,IMG_S_F])\n",
    "\n",
    "# define losses\n",
    "losses = {'model_4':DSSIMObjective(),\n",
    "          'encoder':vae_loss,\n",
    "#           'decoder':'mse',\n",
    "#           'model_2':DSSIMObjective()}\n",
    "#           'refiner':'mae',\n",
    "          'refiner_dssim':DSSIMObjective()}\n",
    "#           'super':'mse',\n",
    "#           'super_dssim':DSSIMObjective()}\n",
    "\n",
    "\n",
    "# compile model\n",
    "AUTOENCODER.compile(optimizer=ADAM,loss=losses)\n",
    "\n",
    "# print summary\n",
    "AUTOENCODER.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'roadtrip-256-1546710766.3303678-RESUME'\n",
    "MODEL_STEPS = 15\n",
    "GIF_STEPS = 1\n",
    "\n",
    "\n",
    "AUTOENCODER.fit(x=[FLAT,K.zeros((184,512))],\n",
    "                y=[FLAT,FLAT_2X,FLAT],\n",
    "                steps_per_epoch=46,\n",
    "                epochs=EPOCHS,\n",
    "                callbacks=[giffer,saver])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
